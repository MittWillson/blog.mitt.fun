{"meta":{"title":"Mitt's Fun space!","subtitle":null,"description":"Mitt 的菜鸡技术博客 | 可恶的Edison","author":"Mitt Willson","url":"https://blog.mitt.fun","root":"/"},"pages":[{"title":"","date":"2021-09-06T10:02:51.785Z","updated":"2021-09-06T10:02:51.785Z","comments":true,"path":"icons/manifest.json","permalink":"https://blog.mitt.fun/icons/manifest.json","excerpt":"","text":"{\"name\":\"App\",\"icons\":[{\"src\":\"/android-icon-36x36.png\",\"sizes\":\"36x36\",\"type\":\"image/png\",\"density\":\"0.75\"},{\"src\":\"/android-icon-48x48.png\",\"sizes\":\"48x48\",\"type\":\"image/png\",\"density\":\"1.0\"},{\"src\":\"/android-icon-72x72.png\",\"sizes\":\"72x72\",\"type\":\"image/png\",\"density\":\"1.5\"},{\"src\":\"/android-icon-96x96.png\",\"sizes\":\"96x96\",\"type\":\"image/png\",\"density\":\"2.0\"},{\"src\":\"/android-icon-144x144.png\",\"sizes\":\"144x144\",\"type\":\"image/png\",\"density\":\"3.0\"},{\"src\":\"/android-icon-192x192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\",\"density\":\"4.0\"}]}"},{"title":"留言板 ლ(°◕‵ƹ′◕ლ)","date":"2021-02-25T12:33:26.000Z","updated":"2021-09-06T10:02:51.785Z","comments":true,"path":"messages/index.html","permalink":"https://blog.mitt.fun/messages/index.html","excerpt":"","text":"这里是畅所欲言的留言板，相信没有朋友的我这里应该是空的呢 :( sad"}],"posts":[{"title":"波卡链/达尔文链治理操作和事件流程笔记 - Updating","slug":"blockchains/substrate-governance-events","date":"2021-08-13T06:19:00.000Z","updated":"2021-08-13T06:19:00.000Z","comments":true,"path":"blockchains/2021/blockchains/substrate-governance-events/","link":"","permalink":"https://blog.mitt.fun/blockchains/2021/blockchains/substrate-governance-events/","excerpt":"pre-post()本文为笔记本记录，主要记录 polkadot 和 darwinia 网络下的治理部分操作和事件触发流程，内容极度不可信，会持续更新 已忽略记录 Deposit 事件, 如非特权操作，Extrinsic 均收取手续费","text":"pre-post()本文为笔记本记录，主要记录 polkadot 和 darwinia 网络下的治理部分操作和事件触发流程，内容极度不可信，会持续更新 已忽略记录 Deposit 事件, 如非特权操作，Extrinsic 均收取手续费 原词词典 中文 原意 提案 Proposal 原象 PreImage 世纪周期 ERA 抵押 Staking 票 Power 公投 Referendum 支出周期 Spend Period 销毁 Burnt 块 Block 延迟 Delay 议会 Council 技术委员会 Tech 财政部 Treasury 小费 Treasury Tip 民主提案 递交提案(无原象) 递交需要 proposal_hash 但无需对内容提交，这是为了避免提案人支出大量费用，可在第三方用 proposal_hash 对提案内容进行验证以证明提案内容一致 Extrinsic: democracy.propose Events: balances.Reserved democracy.Proposed 公投 接下来请移步到 公投部分 议会提案参与提案和投票的是议会成员，之后如果转为公投则为持币的全民 递交提案(无原象) Extrinsic: council.propose Events: council.Proposed 投票 每个成员投票都会产生 Voted 事件 Extrinsic: council.vote Events: council.Voted 结束投票 Extrinsic: technicalcommittee.close Events: technicalcommittee.Closed Member 成员提案并执行 会直接执行，但可能会失败，具体权限方面还得在研究 Extrinsic: council.execute Events: council.MemberExecuted 公投(如果提案投票移交公投的话) 接下来请移步到 公投部分 技术委员会提案 递交提案(无原象) Extrinsic: technicalcommittee.propose Events: technicalcommittee.Proposed 技术委员会投票 Extrinsic: technicalcommittee.vote Events: technicalcommittee.Voted 结束投票 Extrinsic: technicalcommittee.close Events: technicalcommittee.Closed 公投(如果提案投票移交公投的话) 接下来请移步到 公投部分 财政部提案主要用于项目拨款提案，比如宣传、技术费用等 递交提案(无原象) Extrinsic: treasury.propose_spend Events: treasury.Proposed 移交提案 Extrinsic: treasury.reject_proposal 与议会提案相通，接下来会创建议会提案并走流程 接下来请移步到 议会提案部分 公投需要先有提案 提案已递交公投 Events: balances.Unreserved democracy.Tabled democracy.Started 公民投票(开始) Extrinsic: democracy.vote 提交原象 Extrinsic: democracy.note_preimage democracy.note_imminent_preimage Events: democracy.PreimageNoted 取消公投(分支) 这个情况比较特殊，其中有一种情况，议会的提案可以紧急取消公投 紧急取消公投的提案内容是: democracy.emergency_cancel ref_index Events: democracy.Cancelled 系统选票(结束) 只有在公投设定的结束时间(块和延迟)到来才会结束并系统选票 Events: democracy.NotPassed democracy.Passed 提案执行 Events: democracy.PreimageUsed system.CodeUpdated (取决于提案内容) democracy.Excuted","categories":[{"name":"blockchains","slug":"blockchains","permalink":"https://blog.mitt.fun/categories/blockchains/"}],"tags":[{"name":"BlockChains","slug":"BlockChains","permalink":"https://blog.mitt.fun/tags/BlockChains/"},{"name":"Polkadot","slug":"Polkadot","permalink":"https://blog.mitt.fun/tags/Polkadot/"},{"name":"Governance","slug":"Governance","permalink":"https://blog.mitt.fun/tags/Governance/"},{"name":"Darwinia","slug":"Darwinia","permalink":"https://blog.mitt.fun/tags/Darwinia/"},{"name":"Crab","slug":"Crab","permalink":"https://blog.mitt.fun/tags/Crab/"},{"name":"Substrate","slug":"Substrate","permalink":"https://blog.mitt.fun/tags/Substrate/"},{"name":"Notebook","slug":"Notebook","permalink":"https://blog.mitt.fun/tags/Notebook/"}]},{"title":"《预防手残 Velero 集群备份部署 初见！》","slug":"kubernetes/hello-velero","date":"2021-07-18T10:13:00.000Z","updated":"2021-07-18T10:13:00.000Z","comments":true,"path":"service/kubernetes/2021/kubernetes/hello-velero/","link":"","permalink":"https://blog.mitt.fun/service/kubernetes/2021/kubernetes/hello-velero/","excerpt":"pre-post()虽然很早之前就有过用 Velero 对K8S集群备份的了解，但一直没有实施部署，导致由于手欠/BUG对我的K8S造成了一定程度的打击，虽然最后也恢复业务运行了，但着实花了不少时间，而且如果稍微不留神可能会有对整个业务宕机的风险，所以今天就把 Velero 部署起来，并记录一下遇到的一些问题 本文编写使用的环境： OS: macOS BigSurKubernetes: v1.21.2Velero: v1.6.1 我们照着官网的教程来进行安装，由于踩过网上搜的教程的坑，所以这里以我正确安装的步骤为记录，跳过部分坑","text":"pre-post()虽然很早之前就有过用 Velero 对K8S集群备份的了解，但一直没有实施部署，导致由于手欠/BUG对我的K8S造成了一定程度的打击，虽然最后也恢复业务运行了，但着实花了不少时间，而且如果稍微不留神可能会有对整个业务宕机的风险，所以今天就把 Velero 部署起来，并记录一下遇到的一些问题 本文编写使用的环境： OS: macOS BigSurKubernetes: v1.21.2Velero: v1.6.1 我们照着官网的教程来进行安装，由于踩过网上搜的教程的坑，所以这里以我正确安装的步骤为记录，跳过部分坑 由于我采用的是 CLI 安装方法，故可能相对繁琐一些，如果有兴趣的可以使用 Helm 安装，这里给出官方的说明文档 https://github.com/vmware-tanzu/helm-charts/blob/main/charts/velero/README.md 以下操作均在同一命名空间 velero 进行 subject(‘安装 Velero 命令’)macOS with Homebrew1$ brew install velero Windows with Chocolatey1$ choco install velero Linux从 https://github.com/vmware-tanzu/velero/releases 下载 subject(‘准备S3存储凭证’)Velero 支持多种存储方式，可以以装插件的形式扩展，这里我用的是 S3 兼容的 Minio 准备一个凭证文件 credentials-velero 12345$ cat &gt;&gt; credentials-velero &lt;&lt;EOF[default]aws_access_key_id = &lt;accesskey&gt;aws_secret_access_key = &lt;secretkey&gt;EOF Minio 的地址是 http://minio.default.svc:9000 Bucket 是 backups，并且已经赋予了读写权限 subject(‘安装 Velero’)12345678$ velero install --namespace velero \\ --namespace velero \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.2.0 \\ --bucket backups \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=&quot;true&quot;,s3Url=&quot;http://minio.default.svc:9000&quot;,publicUrl=&quot;&quot; 部分参数说明: 名称 说明 provider 存储插件 plugins 额外的插件，这里装的是S3插件，我这里使用的是v1.2.0，建议正式安装前查询下最新的版本或使用 latest 作为 image tag secret-file 凭证文件，提供路径 use-volume-snapshots 启用存储快照，由于我CSI用的本地存储不支持快照，所以设置false backup-location-config S3插件配置 部分插件配置参数说明: 名称 说明 s3Url S3 API域名 publicUrl 公开域名，用于下载用的，我这里因为没有外部访问所以没设置 具体的插件配置信息可以在这里参考 https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/master/backupstoragelocation.md 可以通过命令来检查运行状态是否安装成功 1$ kubectl logs deployment/velero -n velero subject(‘创建一个七牛备份位置’)安装 Velero 时会提供并创建一个默认的备份位置，但是实际情况下可能会有不同的保存地方，我安装的时候采用的是在集群内部安装的 minio，这里我添加 七牛OSS 存储来保证不会因为集群母鸡宕机而导致备份丢失 虽然 Velero 支持插件扩展存储类型支持，但是目前七牛还没有 Velero 的插件，好在七牛也是 兼容S3 的，所以我们只要使用 aws 插件就可以了 在空间管理里找到 S3域名 复制里面的 Endpoint 域名，我的域名是 s3-cn-east-2.qiniucs.com 同样我们需要先创建一个凭证文件 12345$ cat &gt;&gt; credentials-velero-qiniu &lt;&lt;EOF[default]aws_access_key_id = &lt;accesskey&gt;aws_secret_access_key = &lt;secretkey&gt;EOF 然后将凭证创建为一个 secret 1$ kubectl create secret generic -n velero credential-qiniu --from-file=qiniu=credentials-velero-qiniu 这里在 velero 命名空间创建一个名为 credential-qiniu 的 secret，包含了一个 键名 为 qiniu 值为文件 credentials-velero-qiniu 的内容 添加存储位置 1$ velero backup-location create qiniu --bucket k8s-backup --provider aws --config region=cn-east-2,s3ForcePathStyle=&quot;true&quot;,s3Url=https://s3-cn-east-2.qiniucs.com,publicUrl=&quot;https://download.k8s-backup.com&quot; --credential=credential-qiniu=qiniu --default 部分参数说明 名称 说明 credential 凭证secret，格式为 &lt;secret-name&gt;=&lt;key&gt; default 设置为默认存储位置 检查存储位置状态 1$ kubectl describe Backupstoragelocations/qiniu -n velero | grep Phase 显示 Available 就创建成功了 subject(‘小插曲 - 存储位置状态为空或Unavailable’)由于一开始在网上找了其他博客的教程进行安装，所以踩了个坑，这里记录一下，可以跳过这段 先检查 velero 运行日志 1$ kubectl logs -n velero deploy/velero 返回的日志里有很多 error， 我摘抄一段 1time=&quot;2021-07-18T09:56:11Z&quot; level=error msg=&quot;Error getting a backup store&quot; backup-storage-location=qiniu controller=backup-storage-location error=&quot;rpc error: code = Unknown desc = config has invalid keys [credentialsFile]; valid keys are [region s3Url publicUrl kmsKeyId s3ForcePathStyle signatureVersion profile serverSideEncryption insecureSkipTLSVerify bucket prefix caCert]&quot; error.file=&quot;/go/src/github.com/vmware-tanzu/velero-plugin-for-aws/vendor/github.com/vmware-tanzu/velero/pkg/plugin/framework/validation.go:50&quot; error.function=github.com/vmware-tanzu/velero-plugin-for-aws/vendor/github.com/vmware-tanzu/velero/pkg/plugin/framework.validateConfigKeys logSource=&quot;pkg/controller/backup_storage_location_controller.go:100&quot; 这里可以看到，问题出在插件的配置上， config has invalid keys [credentialsFile]， 这里配置 credentialsFile 不合法，但是我们并没有提供这个配置，所以我去找了下 Velero 和 velero-plugin-for-aws 的仓库，并追溯这个参数的来源 通过搜索找到了一个 Velero 的PR， #3443 这个PR提供了 Backupstoragelocations 的外部多凭证支持，并且标记为 v1.6.0 1$ velero version 通过命令查询客户端和服务端版本发现安装的版本是 v1.6.1， 所以是支持这个特性的 接着找 velero-plugin-for-aws 的仓库源码跟踪 credentialsFile 发现也做了适配，但是通过查询 1$ kubectl describe -n velero deploy/velero 在 Init Containers 里发现 velero-velero-plugin-for-aws 的版本是 v1.1.0， 而目前最新版是 v1.2.0，而 credentialsFile 适配也是 v1.2.0 才做的，所以这里是因为网上教程太旧导致安装了旧版出现的问题，通过修改 deploy 的 image 版本解决了这个问题 现在查询状态就是 Avaliable 了，所以这里建议用 latest 作为插件image tag避免安装旧版插件产生其他问题 subject(‘创建一个备份’)到这里所有配置都搞好了，我们就可以开始手动创建一个新备份了，Velero支持选择性备份，可以针对特别的 Namespace, Selector 等进行备份，更详细的介绍可以移步官方文档 我这里创建一个默认备份，会备份集群的所有描述文件 1$ velero backup create first 查询备份状态 1$ velero backup describe first | grep Phase 显示 Completed 就已经备份完成了，可以在Bucket里看到已经上传的备份文件 下载备份 我们还可以通过上述配置的 publicUrl 下载我们的备份文件和备份日志 1$ velero backup download first 然后就下载到本地了，也可以自己手动去对象存储里下载 subject(‘End’)到这里初步的体验就结束了，Velero 后续的还有 Volume 备份（不需要快照支持)，定时备份等， 更详细的功能可以移步官方文档查看","categories":[{"name":"service","slug":"service","permalink":"https://blog.mitt.fun/categories/service/"},{"name":"kubernetes","slug":"service/kubernetes","permalink":"https://blog.mitt.fun/categories/service/kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://blog.mitt.fun/tags/Kubernetes/"},{"name":"K8S","slug":"K8S","permalink":"https://blog.mitt.fun/tags/K8S/"},{"name":"K3S","slug":"K3S","permalink":"https://blog.mitt.fun/tags/K3S/"},{"name":"Velero","slug":"Velero","permalink":"https://blog.mitt.fun/tags/Velero/"}]},{"title":"关于 《我被K8S蹂躏千百遍》的那些事","slug":"kubernetes/issues-of-k8s","date":"2021-04-16T22:49:00.000Z","updated":"2021-04-16T22:49:00.000Z","comments":true,"path":"service/kubernetes/2021/kubernetes/issues-of-k8s/","link":"","permalink":"https://blog.mitt.fun/service/kubernetes/2021/kubernetes/issues-of-k8s/","excerpt":"记录一些我遇到的K8S和Docker的问题以及相应的解决方案","text":"记录一些我遇到的K8S和Docker的问题以及相应的解决方案 问题列表WARN No swap limit support 12345678$ sudo vim /etc/default/grub# 追加以下内容# 如果存在该变量则只追加内容GRUB_CMDLINE_LINUX=&quot;cgroup_enable=memory swapaccount=1&quot;$ sudo update-grub# 需要重启生效 failed to allocate for range 0: no IP addresses available in range set: 172.32.xx.1-172.32.xx.254 123$ rm -rf /var/lib/cni/networks/*$ systemctl restart containerd$ systemctl restart kubelet Warning: Stopping docker.service, but it can still be activated by: docker.socket 1$ rm -rf /lib/systemd/system/docker.socket","categories":[{"name":"service","slug":"service","permalink":"https://blog.mitt.fun/categories/service/"},{"name":"kubernetes","slug":"service/kubernetes","permalink":"https://blog.mitt.fun/categories/service/kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://blog.mitt.fun/tags/Kubernetes/"},{"name":"K8S","slug":"K8S","permalink":"https://blog.mitt.fun/tags/K8S/"},{"name":"K3S","slug":"K3S","permalink":"https://blog.mitt.fun/tags/K3S/"},{"name":"Issues","slug":"Issues","permalink":"https://blog.mitt.fun/tags/Issues/"}]},{"title":"快速扩容 Node 的 podCIDR IP范围","slug":"kubernetes/node-ip-cidr-extends","date":"2021-04-16T22:49:00.000Z","updated":"2021-06-11T20:09:00.000Z","comments":true,"path":"service/kubernetes/2021/kubernetes/node-ip-cidr-extends/","link":"","permalink":"https://blog.mitt.fun/service/kubernetes/2021/kubernetes/node-ip-cidr-extends/","excerpt":"pre-post()为了更好的压榨节点(因为穷买不起第二台), 所以在单台机器上跑了更多的服务，从而导致了更多的pod需要IP分配，而超量则产生了一个异常 failed to allocate for range 0: no IP addresses available in range set, 节点默认安装下来是 /16 的段，而每个节点会分到 /24 段，这意味着每个节点最多可分配 256 个 POD IP，这对于我这种精扒皮来说远远不够，所以就需要对节点的IP段进行扩容，让每个节点能分到更多的IP","text":"pre-post()为了更好的压榨节点(因为穷买不起第二台), 所以在单台机器上跑了更多的服务，从而导致了更多的pod需要IP分配，而超量则产生了一个异常 failed to allocate for range 0: no IP addresses available in range set, 节点默认安装下来是 /16 的段，而每个节点会分到 /24 段，这意味着每个节点最多可分配 256 个 POD IP，这对于我这种精扒皮来说远远不够，所以就需要对节点的IP段进行扩容，让每个节点能分到更多的IP 修改Node由于 Node 信息无法直接修改，这里采取一种变相措施（有一定风险，自己承担），保存Node的配置，修改Node中PodCIDR信息，然后删除掉Node资源，趁Kubelet还没有反应过来就偷偷把修改过的Node资源加回去 保存 node.yaml 1$ kubectl get node test-worker -o yaml &gt; node.yaml 修改 node.yaml 的 podCIDR 123456...spec: podCIDR: 10.32.2.0/22 podCIDRs: - 10.32.2.0/22... 这里我们只需要改为 22 就能让每个节点pod的IP可用数量翻倍得到 1022 个IP，对我来说就足够用了 删除 node 信息 1$ kubectl delete node test-worker 导入节点 1$ kubectl apply -f ./node.yaml 然后就完成了，创建新pod的时候，就会从podCIDR和IP缓存文件信息里找可用IP创建，（不过奇怪的是我的pod并没有超过200，但依然出现IP池短缺的问题，可能是flannel在销毁pod后并没有销毁IP信息，导致IP一直被占用）","categories":[{"name":"service","slug":"service","permalink":"https://blog.mitt.fun/categories/service/"},{"name":"kubernetes","slug":"service/kubernetes","permalink":"https://blog.mitt.fun/categories/service/kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://blog.mitt.fun/tags/Kubernetes/"},{"name":"K8S","slug":"K8S","permalink":"https://blog.mitt.fun/tags/K8S/"},{"name":"K3S","slug":"K3S","permalink":"https://blog.mitt.fun/tags/K3S/"},{"name":"Issues","slug":"Issues","permalink":"https://blog.mitt.fun/tags/Issues/"},{"name":"Flannel","slug":"Flannel","permalink":"https://blog.mitt.fun/tags/Flannel/"}]},{"title":"利用 Cloudflare Workers 创建一个跳转友链服务(乞丐级简易版)","slug":"script/cf-workers-redirect-to-friends","date":"2021-03-03T12:33:09.000Z","updated":"2021-03-03T12:33:09.000Z","comments":true,"path":"scripts/2021/script/cf-workers-redirect-to-friends/","link":"","permalink":"https://blog.mitt.fun/scripts/2021/script/cf-workers-redirect-to-friends/","excerpt":"pre-post()水了几篇博文胆子肥了起来，开始到处交换友链，但是想创建一个简便易维护的友链服务，前期先实现能跳转到目标链接，后期想能扩展扩展支持监控友链目标网站的有效状态，结合通知来实现报警，但本篇只实现了 能用 （逃，如果没咕咕咕的话以后会升级代码并支持更高级的功能然后再水一篇发布新文章来介绍。 更新日期: 2021-03-03","text":"pre-post()水了几篇博文胆子肥了起来，开始到处交换友链，但是想创建一个简便易维护的友链服务，前期先实现能跳转到目标链接，后期想能扩展扩展支持监控友链目标网站的有效状态，结合通知来实现报警，但本篇只实现了 能用 （逃，如果没咕咕咕的话以后会升级代码并支持更高级的功能然后再水一篇发布新文章来介绍。 更新日期: 2021-03-03 reuqired(‘依赖’)需要做以下步骤 创建一个 KV 仓库 创建一个 Worker 将 KV 关联到 Worker 并命名为 FRIENDS 将 代码本体 复制到 Worker code(‘代码本体’)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657addEventListener(&#x27;fetch&#x27;, event =&gt; &#123; event.respondWith(handleRequest(event.request))&#125;)const domains = &#123; get(name) &#123; return FRIENDS.get(name); &#125;&#125;;/** * Respond to the request * @param &#123;Request&#125; request */async function handleRequest(request) &#123; const url = new URL(request.url); const host = url.host; const path = url.pathname; let friendName = (host.match(/^(.*?)(?=\\.)/g) || [])[0]; let redirect = null; // friends.mitt.fun/&#123;name&#125; if (friendName === &#x27;friends&#x27;) &#123; const r = /^\\/?([^\\/]*)\\/?(.*?)$/g.exec(path) || []; if (path.match(/^\\/?$/g)) &#123; return indexPage(); &#125; friendName = r[1]; const fUrl = await domains.get(friendName); if (fUrl) &#123; redirect = fUrl.replace(/\\/$/g, &#x27;&#x27;) + &#x27;/&#x27; + (r[2] || &#x27;&#x27;); &#125; &#125; else &#123; const fUrl = await domains.get(friendName); if (fUrl) &#123; redirect = fUrl.replace(/\\/$/g, &#x27;&#x27;) + path; &#125; &#125; if (!redirect) &#123; return noFriendAnyMore(friendName); &#125; return redirectTo(friendName, redirect);&#125;function noFriendAnyMore(friendName) &#123; return new Response(`Sorry, $&#123;friendName || &#x27;this one&#x27;&#125; is not be my firends anymore.`, &#123;status: 200&#125;);&#125;function redirectTo(friendName, url) &#123; return new Response(&#x27;You are going to Mitt\\&#x27;s friend\\&#x27;s area&#x27;, &#123;status: 302, headers: &#123;location: url&#125;&#125;)&#125;function indexPage() &#123; return new Response(`Hi there, this is Mitt&#x27;s Friends redirector, nice to see you :)`)&#125; subject(‘目前功能介绍’)目前实现以下两种访问方式 https://{KEY}.mitt.fun 通过域名最底层的名字 &#123;KEY&#125; 来关联 KV 服务中的 Key 值 你可以定义任何层级的域名，只要最外面是 &#123;KEY&#125; 就可以了, 比如 https://&#123;KEY&#125;.friends.of.mine.mitt.fun 也同样会匹配 https://friends.mitt.fun/{KEY} 通过访问 Path 的第一层 &#123;KEY&#125; 来关联 KV 服务中的 Key 值 以上的两种方式都支持传递路径，即通过在后面跟上 /xxxx 来转发到目标网址的 /xxxx subject(‘example’)自定义域名方式转发: 关联 edison.mitt.fun 域名到创建的 Worker 在 KV 服务中填写 Key 为 edison, Value 为 https://www.wevg.org 测试访问 https://edison.mitt.fun 会302跳转到 https://www.wevg.org 测试访问 https://edison.mitt.fun/friends/ 会302跳转到 https://www.wevg.org/friends/ 目录方式转发: 关联 friends.mitt.fun 域名到创建的 Worker 在 KV 服务中填写 Key 为 peter, Value 为 https://pzhang.net/ 测试访问 https://friends.mitt.fun/peter 会302跳转到 https://pzhang.net/ 测试访问 https://friends.mitt.fun/peter/archives/ 会302跳转到 https://pzhang.net/archives/ end()又水了一篇，好唉。 (别问我为啥某人不在 Links","categories":[{"name":"scripts","slug":"scripts","permalink":"https://blog.mitt.fun/categories/scripts/"}],"tags":[{"name":"Cloudflare","slug":"Cloudflare","permalink":"https://blog.mitt.fun/tags/Cloudflare/"},{"name":"Scripts","slug":"Scripts","permalink":"https://blog.mitt.fun/tags/Scripts/"},{"name":"Friends","slug":"Friends","permalink":"https://blog.mitt.fun/tags/Friends/"}]},{"title":"域名解析使MX和CNAME共存 解决域名邮箱收发件问题","slug":"domain/dns-mx-cname-coexist","date":"2021-02-26T20:12:22.000Z","updated":"2021-02-27T13:40:22.000Z","comments":true,"path":"dns/2021/domain/dns-mx-cname-coexist/","link":"","permalink":"https://blog.mitt.fun/dns/2021/domain/dns-mx-cname-coexist/","excerpt":"pre-post()因为我自己搞了一个域名邮箱，用的腾讯企业邮箱，之前为了上国内CDN从Cloudflare 迁移出来换成 Cloudflare Partner，用 DNSPOD 实现 GEODNS，国内用 百度CDN，国外用 Cloudflare，通过 CNAME 解析指向，但是后面发现一个问题就是域名邮箱突然无法正常收发邮件了，网上通过谷歌和其他博主的博文得知这是DNS的特性导致的(参考5)，如果想要解决就只能A+MX的共存方式，但是显然这种方式对我来说弊端很多，另一种方案就是将网站解析到www子域名，把@腾出来给MX，或者将域名邮箱解析到 mail 子域名，虽然对大部分人来说可行，但是对我有点强迫症，我就想用@，毕竟chrome都模糊化www和@了，自然是@更适合一点，接下来的方法就是我自己瞎鼓捣出来的，但是又没有查询相关资料证明100%不会出问题，但到目前来看还是很稳的，也不确定放到其他DNS服务商以及其他CDN服务商是否同样有效。","text":"pre-post()因为我自己搞了一个域名邮箱，用的腾讯企业邮箱，之前为了上国内CDN从Cloudflare 迁移出来换成 Cloudflare Partner，用 DNSPOD 实现 GEODNS，国内用 百度CDN，国外用 Cloudflare，通过 CNAME 解析指向，但是后面发现一个问题就是域名邮箱突然无法正常收发邮件了，网上通过谷歌和其他博主的博文得知这是DNS的特性导致的(参考5)，如果想要解决就只能A+MX的共存方式，但是显然这种方式对我来说弊端很多，另一种方案就是将网站解析到www子域名，把@腾出来给MX，或者将域名邮箱解析到 mail 子域名，虽然对大部分人来说可行，但是对我有点强迫症，我就想用@，毕竟chrome都模糊化www和@了，自然是@更适合一点，接下来的方法就是我自己瞎鼓捣出来的，但是又没有查询相关资料证明100%不会出问题，但到目前来看还是很稳的，也不确定放到其他DNS服务商以及其他CDN服务商是否同样有效。 subject(‘实现目标’)能够使用 DNSPOD 使 pwecho.com 支持境内境外不同CDN服务 Cloudflare 和 BaiduCDN，同时兼容 腾讯企业邮箱 subject(‘当前配置’) Cloudflare Partner Name Type Record Remark @ CNAME s-ip.pwecho.com 网站服务器 使用 s-ip.pwecho.com 方便更改IP，CDN需要CNAME指向 s-ip.pwecho.com DNSPOD Name Type Record Region Remark s-ip A x.x.x.x 网站服务器源站IP @ MX mx1.qq.com 默认 腾讯企业邮箱 @ MX mx2.qq.com 默认 腾讯企业邮箱 @ CNAME pwecho.com.cdn.cloudflare.com 境外 Cloudflare @ CNAME pwecho.com.a.bdydns.com 境内 BaiduCDN 虽然 DNSPOD 支持MX和CNAME同时记录，会提示可能会产生不兼容问题，但可以忽略，实际测试会出现无法正常返回MX的情况 subject(‘解决方案’)目前的解决方案是，单独剥离一个CNAME，境外让 Cloudflare 负责提供MX和A记录，境内让 DNSPOD 提供，以下是修改方案 Cloudflare Partner Name Type Record Remark @ CNAME s-ip.pwecho.com 网站服务器 @ MX mx1.qq.com 腾讯企业邮箱 @ MX mx2.qq.com 腾讯企业邮箱 DNSPOD Name Type Record Region Remark s-ip A x.x.x.x 网站服务器源站IP root.cn-bd-cdn CNAME pwecho.com.a.bdydns.com 默认 BaiduCDN root.cn-bd-cdn MX mx1.qq.com 默认 腾讯企业邮箱 root.cn-bd-cdn MX mx2.qq.com 默认 腾讯企业邮箱 @ CNAME pwecho.com.cdn.cloudflare.com 境外 Cloudflare @ CNAME root.cn-bd-cdn.pwecho.com 境内 通过以上配置查询DNS记录即可得出结果 中国地区 123456789101112131415161718192021$ dig MX pwecho.com; &lt;&lt;&gt;&gt; DiG 9.14.8 &lt;&lt;&gt;&gt; MX pwecho.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 11135;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;pwecho.com. IN MX;; ANSWER SECTION:pwecho.com. 597 IN CNAME root.cn-bd-cdn.pwecho.com.root.cn-bd-cdn.pwecho.com. 597 IN MX 5 mxbiz1.qq.com.root.cn-bd-cdn.pwecho.com. 597 IN MX 10 mxbiz2.qq.com.;; Query time: 10 msec;; SERVER: 192.168.50.1#53(192.168.50.1);; WHEN: Sat Feb 27 14:11:41 ;; MSG SIZE rcvd: 117 1234567891011121314151617$ dig CNAME pwecho.com; &lt;&lt;&gt;&gt; DiG 9.14.8 &lt;&lt;&gt;&gt; CNAME pwecho.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 63349;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;pwecho.com. IN CNAME;; ANSWER SECTION:pwecho.com. 563 IN CNAME root.cn-bd-cdn.pwecho.com.;; Query time: 76 msec;; SERVER: 192.168.50.1#53(192.168.50.1);; WHEN: Sat Feb 27 14:12:10 ;; MSG SIZE rcvd: 57 12345678910111213141516171819202122$ dig A pwecho.com; &lt;&lt;&gt;&gt; DiG 9.14.8 &lt;&lt;&gt;&gt; A pwecho.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 12359;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;pwecho.com. IN A;; ANSWER SECTION:pwecho.com. 37 IN CNAME root.cn-bd-cdn.pwecho.com.root.cn-bd-cdn.pwecho.com. 37 IN CNAME pwecho.com.a.bdydns.com.pwecho.com.a.bdydns.com. 37 IN CNAME opencdncloud.jomodns.com.opencdncloud.jomodns.com. 37 IN A 101.72.249.35;; Query time: 9 msec;; SERVER: 192.168.50.1#53(192.168.50.1);; WHEN: Sat Feb 27 14:15:33 ;; MSG SIZE rcvd: 153 境外地区 123456789101112131415161718192021$ dig MX pwecho.com; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; MX pwecho.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 48915;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 1232;; QUESTION SECTION:;pwecho.com. IN MX;; ANSWER SECTION:pwecho.com. 600 IN MX 10 mxbiz2.qq.com.pwecho.com. 600 IN MX 5 mxbiz1.qq.com.;; Query time: 276 msec;; SERVER: 1.1.1.1#53(1.1.1.1);; WHEN: Sat Feb 27 07:14:54 CET 2021;; MSG SIZE rcvd: 88 1234567891011121314151617181920$ dig CNAME pwecho.com; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; CNAME pwecho.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 55778;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 1232;; QUESTION SECTION:;pwecho.com. IN CNAME;; ANSWER SECTION:pwecho.com. 600 IN CNAME pwecho.com.cdn.cloudflare.net.;; Query time: 276 msec;; SERVER: 1.1.1.1#53(1.1.1.1);; WHEN: Sat Feb 27 07:14:43 CET 2021;; MSG SIZE rcvd: 82 1234567891011121314151617181920212223$ dig A pwecho.com; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; A pwecho.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 35366;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 1232;; QUESTION SECTION:;pwecho.com. IN A;; ANSWER SECTION:pwecho.com. 600 IN CNAME pwecho.com.cdn.cloudflare.net.pwecho.com.cdn.cloudflare.net. 300 IN A 104.26.6.173pwecho.com.cdn.cloudflare.net. 300 IN A 104.26.7.173pwecho.com.cdn.cloudflare.net. 300 IN A 172.67.69.29;; Query time: 280 msec;; SERVER: 1.1.1.1#53(1.1.1.1);; WHEN: Sat Feb 27 07:17:11 CET 2021;; MSG SIZE rcvd: 130 从上面结果可以看出，pwecho.com.cdn.cloudflare.net 本身返回的就是 A 记录，而它做的只是将你的 MX 记录也加到他的 CNAME 域名解析中，所以完全没有问题，而 DNSPOD 虽然还是返回 CNAME 来解析 A 记录，但是实测多次并没有发现出现查询 MX, CNAME 混乱返回的情况, 但同样我也尚未能提供资料证明不会出现这个问题。 ⚠️ 补充测试结果: 通过 https://www.boce.com/dns/ 测试发现全国查询的时候会有部分地区随机返回 CNAME 记录，所以这个方法还是不稳定，Cloudflare 那种实现对境外邮箱来说则完全没有问题 其实这里还有个发现，即便是国内的 QQ邮箱, 腾讯企业邮箱, 163邮箱 或者是国外的 Gmail, Outlook 都会查询出境外的MX记录，即便境内没设置MX记录也可，所以如果不放心其实可以直接干掉境内的MX记录，只让 Cloudflare 返回 MX 记录也可以稳定运作 (不打包票，境内还有很多邮箱没有做测试 参考资料 域名解析的 MX 和 CNAME 记录冲突 - 谢先斌的博客 解析记录冲突规则 - 阿里云文档 三种方法解决域名解析的主机记录冲突 - Erdong’ Blog 域名 MX 记录与 CNAME 冲突 - 老鬼的博客 RCF 1034 - 下载PDF文件 DNS using CNAMEs breaks MX records? - StackExchange","categories":[{"name":"dns","slug":"dns","permalink":"https://blog.mitt.fun/categories/dns/"}],"tags":[{"name":"DNS","slug":"DNS","permalink":"https://blog.mitt.fun/tags/DNS/"},{"name":"域名邮箱","slug":"域名邮箱","permalink":"https://blog.mitt.fun/tags/%E5%9F%9F%E5%90%8D%E9%82%AE%E7%AE%B1/"},{"name":"企业邮箱","slug":"企业邮箱","permalink":"https://blog.mitt.fun/tags/%E4%BC%81%E4%B8%9A%E9%82%AE%E7%AE%B1/"},{"name":"GEODNS","slug":"GEODNS","permalink":"https://blog.mitt.fun/tags/GEODNS/"},{"name":"区域解析","slug":"区域解析","permalink":"https://blog.mitt.fun/tags/%E5%8C%BA%E5%9F%9F%E8%A7%A3%E6%9E%90/"},{"name":"MX共存","slug":"MX共存","permalink":"https://blog.mitt.fun/tags/MX%E5%85%B1%E5%AD%98/"},{"name":"MX","slug":"MX","permalink":"https://blog.mitt.fun/tags/MX/"},{"name":"CNAME","slug":"CNAME","permalink":"https://blog.mitt.fun/tags/CNAME/"},{"name":"Cloudflare","slug":"Cloudflare","permalink":"https://blog.mitt.fun/tags/Cloudflare/"},{"name":"Dnspod","slug":"Dnspod","permalink":"https://blog.mitt.fun/tags/Dnspod/"},{"name":"百度CDN","slug":"百度CDN","permalink":"https://blog.mitt.fun/tags/%E7%99%BE%E5%BA%A6CDN/"}]},{"title":"关于《Kubernetes 混合云用Kilo解决NAT节点通讯的问题》的那件事","slug":"kubernetes/k8s-混合云组网","date":"2021-02-25T05:07:29.000Z","updated":"2021-02-25T05:07:29.000Z","comments":true,"path":"service/kubernetes/2021/kubernetes/k8s-混合云组网/","link":"","permalink":"https://blog.mitt.fun/service/kubernetes/2021/kubernetes/k8s-%E6%B7%B7%E5%90%88%E4%BA%91%E7%BB%84%E7%BD%91/","excerpt":"pre-post()最近有在学习K8S相关，同时也将自己的所有服务全部都迁移到了K8S集群上，感受到K8S强大的同时也能明显感受到对于我这种一般用户环境的部署不是很友好，例如建议的高可用集群至少要有 三台 Master, 三台 Worker, 对我这种 穷逼 普通玩家来说是很高昂的，但经过几次实践把生产环境搞炸几次后觉得这是有必要的(哭)，但我的想法是用K8S作为我主要部署方式，所以还是非常高昂的，像我这种为了能够管理所有节点但是又不会重度使用K8S的，这里推荐 K3S 来代替K8S作为要求不高的生产环境，不过这篇文章还是用完整的K8S来做，但理论上 K3S 是完全通用的。","text":"pre-post()最近有在学习K8S相关，同时也将自己的所有服务全部都迁移到了K8S集群上，感受到K8S强大的同时也能明显感受到对于我这种一般用户环境的部署不是很友好，例如建议的高可用集群至少要有 三台 Master, 三台 Worker, 对我这种 穷逼 普通玩家来说是很高昂的，但经过几次实践把生产环境搞炸几次后觉得这是有必要的(哭)，但我的想法是用K8S作为我主要部署方式，所以还是非常高昂的，像我这种为了能够管理所有节点但是又不会重度使用K8S的，这里推荐 K3S 来代替K8S作为要求不高的生产环境，不过这篇文章还是用完整的K8S来做，但理论上 K3S 是完全通用的。 subject(‘实现目标’)利用 Kilo 将带有公网IP的NAT云服务器组成K8S集群，支持 P2P, DDNS (要求有公网IP且需要端口映射) subject(‘Kilo介绍’)Kilo 是一个通过 Wireguard 用于建立混合云网络的工具 subject(‘现有问题’)我的服务器构成是这样的(以下IP皆为虚拟): Name ifc-IP Location Role Nat k8s-master 123.123.123.123 Hetzner Master NO cn-sh01-node 10.0.0.4 QCloud Worker YES cn-hz01-node 192.168.1.120 Home Worker YES 可以看到我的三台服务器都不是同一个网段甚至都不是同一个服务商的。所以会有几个问题 kube-proxy 无法正常工作转发流量 metrics 采集无法工作 logs/shell 无法工作 subject(‘解决问题’)之前是用 Weave 来作为CNI的，然后为了解决 kube-proxy 的一些问题换成了 Flannel, 网上查找一些资料和issues以后发现如果要解决这个问题，就得先将所有节点连起来，连起来的方法就是VPN，然后让通讯流量走VPN接口即可解决，但是按常规理解VPN流量是需要中心服务器转发流量的，那就会导致所有流量转发到同一台服务器，压力和延迟也会非常大不符合需求，然后我去搜了P2P VPN发现 Wireguard 是支持Peer2Peer的，顺势在某个issue里看到有人提到用 Kilo 进行自动组网，并且 Kilo 是支持在 Flannel 之上运行的 端口通讯检查我是采用 Flannel+Calico+Kilo 的方式设置网络的，所以需要以下端口放通 Port Range Protocol Remark 8285 UDP Flannel 8472 UDP Flannel 51820 UDP Wireguard 默认端口 10250 TCP Kubelet API 30000-32767 TCP+UDP NodePort 服务端口 安装 KiloKubeadm 1$ kubectl apply -f https://raw.githubusercontent.com/squat/kilo/master/manifests/kilo-kubeadm-flannel.yaml 如果要卸载，直接 kubectl delete 就好了 配置外部连接IP由于目前尚未支持 NAT to NAT(理论可以实现)，所以每个Node都必须具备外部(公网)访问条件，但是你会发现一件事，每个Node都只会拿到网卡的IP，它没办法发现你的外部IP 所以就像 Flannel 通过 flannel.alpha.coreos.com/public-ip-overwrite 来覆写外部通讯IP一样 Kilo 同样也提供了 kilo.squat.ai/force-endpoint 来指定外部通讯连接点，格式是 &quot;IP:PORT&quot; 或者 &quot;[DOMAIN]:PORT&quot;，是的，它支持域名，所以就可以实现我们 HOME worker的DDNS需求，只要防火墙放通端口或者路由器转发端口就可以自由通讯了 编辑 cn-hz01-node 节点，在 metadata.annotations 里加入 kilo.squat.ai/force-endpoint: &#39;[home.mydomain.com]:51820&#39;，然后过一会通过 kubectl desc node cn-hz01-node 就可以看到它自动解析了域名并且添加了一个新的 kilo.squat.ai/endpoint annotation，值为你域名指向的IP 同样此方法去更改 cn-sh01-node 的 annotations，过一会就可以看到 metrics 信息已经正常显示了(前提是你已经部署了metrics采集) 建议采用 Flannel 的 vxlan 作为后端，不采用 IPSec 等加密后端避免不必要的二次开销 关于N2N实现 目前 Kilo 的N2N实现还在讨论并且已经在计划适配，具体可以在这里看到 https://github.com/squat/kilo/issues/109 subject(‘不通过VPN外部直连的可能性’)说个题外话，假如全部NODE都是有公网IP但是会有一层NAT的能不能正常通讯呢，这个读过一点Kubernetes的代码，这个是有可能的，就是更改Node的 status.addresses 添加一个 Type 为 ExternalIP 的IP地址，但是这里有个问题就是，你没办法直接编辑或者patch一个node的status值，那么这个addresses是怎么来的呢？ addresses 实际上是通过 cloud-provider 设置的，他读取你的网卡并将其IP设置为 InternalIP 的address, 如果你是GCE, Azure等，他们会去跟平台通讯获取你机器的外网绑定网卡信息，并且设置为 ExternalIP，而 metrics-server 的默认启动参数 --kubelet-preferred-address-types=ExternalIP,InternalIP,Hostname 定义了它会尝试去和外部IP、内部IP、主机名进行通讯，所以如果你是通过云服务商的k8s托管，那么它就会自动设置外部IP，但是手工设置是行不通的，所以如果自己写一个 “Fake” cloud-provider 的话也许也行得通，但其实价值就很低了，不如直接VPN组网来的实在。","categories":[{"name":"service","slug":"service","permalink":"https://blog.mitt.fun/categories/service/"},{"name":"kubernetes","slug":"service/kubernetes","permalink":"https://blog.mitt.fun/categories/service/kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://blog.mitt.fun/tags/Kubernetes/"},{"name":"K8S","slug":"K8S","permalink":"https://blog.mitt.fun/tags/K8S/"},{"name":"K3S","slug":"K3S","permalink":"https://blog.mitt.fun/tags/K3S/"},{"name":"Wireguard","slug":"Wireguard","permalink":"https://blog.mitt.fun/tags/Wireguard/"},{"name":"Kilo","slug":"Kilo","permalink":"https://blog.mitt.fun/tags/Kilo/"},{"name":"Peer2Peer","slug":"Peer2Peer","permalink":"https://blog.mitt.fun/tags/Peer2Peer/"}]},{"title":"Docker - 给每个容器配置公网IP","slug":"docker/Docker-给每个容器配置公网IP","date":"2018-05-01T12:28:08.000Z","updated":"2021-02-25T11:57:00.000Z","comments":true,"path":"service/2018/docker/Docker-给每个容器配置公网IP/","link":"","permalink":"https://blog.mitt.fun/service/2018/docker/Docker-%E7%BB%99%E6%AF%8F%E4%B8%AA%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE%E5%85%AC%E7%BD%91IP/","excerpt":"pre-post()为了方便部署流量转发服务，采用了Docker来干这个事，一般情况下一台节点只有一个IP，使用端口映射或者用参数 --network host 直接使用主机的网络来监听和转发是没有问题的，但是有一些节点会存在多个或一个段的外部IP，这时候虽然 IN 方向没有问题，但是 OUT 方向只会采用主机设置的默认路由来访问，就导致了转发的流量都使用同一个IP转发出去了，这是不对的，所以接下来就研究如何才能自定义出口流量，这篇文章则采用给容器配置外部IP的方式实现。 此方法适合非NAT主机","text":"pre-post()为了方便部署流量转发服务，采用了Docker来干这个事，一般情况下一台节点只有一个IP，使用端口映射或者用参数 --network host 直接使用主机的网络来监听和转发是没有问题的，但是有一些节点会存在多个或一个段的外部IP，这时候虽然 IN 方向没有问题，但是 OUT 方向只会采用主机设置的默认路由来访问，就导致了转发的流量都使用同一个IP转发出去了，这是不对的，所以接下来就研究如何才能自定义出口流量，这篇文章则采用给容器配置外部IP的方式实现。 此方法适合非NAT主机 subject(‘实现目标’)给每个 Docker 容器都配置不同的公网IP出口 solution(‘附加网络到容器’)12345678# 从 eth1 接口删除IP$ sudo ip addr del 192.168.33.10/24 dev eth1# 创建一个桥接名称为 “docker1” 的 shared_nw 网络$ sudo docker network create --driver bridge --subnet=192.168.33.0/24 --gateway=192.168.33.10 --opt &quot;com.docker.network.bridge.name&quot;=&quot;docker1&quot; shared_nw# 添加 “docker1” 到 eth1$ sudo brctl addif docker1 eth1 其中 192.168.33.0/24 是子网段, 192.168.33.10 是IP地址 123# 通过 curl 容器来验证IP结果$ docker run -it curlimages/curl --net shared_nw --ip 192.168.33.11 ip.sb# 输出结果: 192.168.33.11 这样这个容器本身的IP就变成了外部IP 弊端采用这种方法会把主机的IP剥离，如果你没有其他接入网卡的话会导致断网无法外部进入，只能通过 Console 来调整 参考来源 https://forums.docker.com/t/public-accessible-ip-in-container-like-bridge-network-in-virtualbox/3668/6 https://qiita.com/kjtanaka/items/f16757c1f0cc86ff225b solution(‘桥接网络IP’)12# 创建名为 &quot;pnet01&quot; 的 macvlan 网络$ docker network create -d macvlan --subnet 23.89.4.0/24 --gateway 23.89.4.1 -o parent=ens3 --aux-address=&quot;master=23.89.4.205&quot; pnet01 其中 23.89.4.0/24 是子网段, 23.89.4.1 是网关地址, ens3 是网卡接口名称, 23.89.4.205 是外部IP 123# 通过 curl 容器来验证IP结果$ docker run -it curlimages/curl --net pnet01 --ip 23.89.4.207 ip.sb# 输出结果: 23.89.4.207 其中 23.89.4.207 是要分配给容器的同网段的IP 其他信息主机 123456789101112131415# ifconfigens3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 23.89.4.205 netmask 255.255.255.0 broadcast 23.89.4.255 inet6 fe80::216:3cff:fe7e:fbeb prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:16:3c:7e:fb:eb txqueuelen 1000 (Ethernet) RX packets 96281 bytes 7457992 (7.1 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5325 bytes 1603561 (1.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# routeDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 1.4-89-23.rdns. 0.0.0.0 UG 0 0 0 ens3localnet 0.0.0.0 255.255.255.0 U 0 0 0 ens3172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 容器内 12345678910111213# ifconfigeth0 Link encap:Ethernet HWaddr 02:42:17:59:04:CF inet addr:23.89.4.207 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:27457 errors:0 dropped:0 overruns:0 frame:0 TX packets:579 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1839527 (1.7 MiB) TX bytes:81473 (79.5 KiB)# routeDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 1.4-89-23.rdns. 0.0.0.0 UG 0 0 0 eth023.89.4.0 * 255.255.255.0 U 0 0 0 eth0 弊端仅适用于拥有一个段的IP的服务器，采用这种方式的主机会保留一个IP用于通讯和桥接，然后分配同网段的其他IP给容器，如果有不同网段和网关的IP，就需要同样创建第二个网络 参考来源 https://docs.docker.com/network/network-tutorial-macvlan/#bridge-example https://www.aquasec.com/wiki/display/containers/Docker+Networking+101 subject(‘End’)这篇文章内容是 2018 年写的，一直咕咕到 2021 年才补上，对现在的我来说对容器以及网络方面有了新的理解，所以这篇文章内容其实也算是有点 过时 了，还有一个方法是采用 iptables NAT进行实现，但是人工维护显然有些繁琐，虽然我没有再这个方向继续研究了，但是希望能给有需要的人一个参考。","categories":[{"name":"service","slug":"service","permalink":"https://blog.mitt.fun/categories/service/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.mitt.fun/tags/Docker/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-01-28T17:30:37.000Z","updated":"2021-02-25T03:57:00.000Z","comments":true,"path":"misc/2018/hello-world/","link":"","permalink":"https://blog.mitt.fun/misc/2018/hello-world/","excerpt":"","text":"Hello Everyone, I’m Mitt, and I wish you all Have fun every day!","categories":[],"tags":[{"name":"Milestone","slug":"Milestone","permalink":"https://blog.mitt.fun/tags/Milestone/"}]}],"categories":[{"name":"blockchains","slug":"blockchains","permalink":"https://blog.mitt.fun/categories/blockchains/"},{"name":"service","slug":"service","permalink":"https://blog.mitt.fun/categories/service/"},{"name":"kubernetes","slug":"service/kubernetes","permalink":"https://blog.mitt.fun/categories/service/kubernetes/"},{"name":"scripts","slug":"scripts","permalink":"https://blog.mitt.fun/categories/scripts/"},{"name":"dns","slug":"dns","permalink":"https://blog.mitt.fun/categories/dns/"}],"tags":[{"name":"BlockChains","slug":"BlockChains","permalink":"https://blog.mitt.fun/tags/BlockChains/"},{"name":"Polkadot","slug":"Polkadot","permalink":"https://blog.mitt.fun/tags/Polkadot/"},{"name":"Governance","slug":"Governance","permalink":"https://blog.mitt.fun/tags/Governance/"},{"name":"Darwinia","slug":"Darwinia","permalink":"https://blog.mitt.fun/tags/Darwinia/"},{"name":"Crab","slug":"Crab","permalink":"https://blog.mitt.fun/tags/Crab/"},{"name":"Substrate","slug":"Substrate","permalink":"https://blog.mitt.fun/tags/Substrate/"},{"name":"Notebook","slug":"Notebook","permalink":"https://blog.mitt.fun/tags/Notebook/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://blog.mitt.fun/tags/Kubernetes/"},{"name":"K8S","slug":"K8S","permalink":"https://blog.mitt.fun/tags/K8S/"},{"name":"K3S","slug":"K3S","permalink":"https://blog.mitt.fun/tags/K3S/"},{"name":"Velero","slug":"Velero","permalink":"https://blog.mitt.fun/tags/Velero/"},{"name":"Issues","slug":"Issues","permalink":"https://blog.mitt.fun/tags/Issues/"},{"name":"Flannel","slug":"Flannel","permalink":"https://blog.mitt.fun/tags/Flannel/"},{"name":"Cloudflare","slug":"Cloudflare","permalink":"https://blog.mitt.fun/tags/Cloudflare/"},{"name":"Scripts","slug":"Scripts","permalink":"https://blog.mitt.fun/tags/Scripts/"},{"name":"Friends","slug":"Friends","permalink":"https://blog.mitt.fun/tags/Friends/"},{"name":"DNS","slug":"DNS","permalink":"https://blog.mitt.fun/tags/DNS/"},{"name":"域名邮箱","slug":"域名邮箱","permalink":"https://blog.mitt.fun/tags/%E5%9F%9F%E5%90%8D%E9%82%AE%E7%AE%B1/"},{"name":"企业邮箱","slug":"企业邮箱","permalink":"https://blog.mitt.fun/tags/%E4%BC%81%E4%B8%9A%E9%82%AE%E7%AE%B1/"},{"name":"GEODNS","slug":"GEODNS","permalink":"https://blog.mitt.fun/tags/GEODNS/"},{"name":"区域解析","slug":"区域解析","permalink":"https://blog.mitt.fun/tags/%E5%8C%BA%E5%9F%9F%E8%A7%A3%E6%9E%90/"},{"name":"MX共存","slug":"MX共存","permalink":"https://blog.mitt.fun/tags/MX%E5%85%B1%E5%AD%98/"},{"name":"MX","slug":"MX","permalink":"https://blog.mitt.fun/tags/MX/"},{"name":"CNAME","slug":"CNAME","permalink":"https://blog.mitt.fun/tags/CNAME/"},{"name":"Dnspod","slug":"Dnspod","permalink":"https://blog.mitt.fun/tags/Dnspod/"},{"name":"百度CDN","slug":"百度CDN","permalink":"https://blog.mitt.fun/tags/%E7%99%BE%E5%BA%A6CDN/"},{"name":"Wireguard","slug":"Wireguard","permalink":"https://blog.mitt.fun/tags/Wireguard/"},{"name":"Kilo","slug":"Kilo","permalink":"https://blog.mitt.fun/tags/Kilo/"},{"name":"Peer2Peer","slug":"Peer2Peer","permalink":"https://blog.mitt.fun/tags/Peer2Peer/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.mitt.fun/tags/Docker/"},{"name":"Milestone","slug":"Milestone","permalink":"https://blog.mitt.fun/tags/Milestone/"}]}